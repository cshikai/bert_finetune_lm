
data:
    path: '/src/data'

---
model:
    use_uncased: False
    task: "PRETRAIN"
    sequence_length: 256
---
train:
    num_epochs: 3
    seed: 123
    n_gpu: 1
    num_workers: 8
    accelerator: 'ddp'
    #epochs: 40
    batch_size: 12
    lr: 0.000005 # change
    auto_lr: False

    checkpoint_dir: '/src/trained_models/'
    save_top_k : -1 #number of top k models to save while training
    model_save_period: 1 # models are saved at the end of every <model_save_period> epoch
    log_every_n_steps : 2 #logs are written every <log_every_n_steps> steps
