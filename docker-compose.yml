version: '3.4'

services:
  model:
    
    # command: python main.py
    command: python qa_main.py
    runtime: nvidia
    build: ./model 
    # image: 
    stdin_open: true
    tty: true
    volumes:
      - ./model/src:/src
    # ports:
    #   - 8887:8887
    #   - 6006:6006
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 32gb # set upper limit for how much shared memory container can use
    #runtime: nvidia

    # environment:
    #   NVIDIA_VISIBLE_DEVICES: 0,1
    # runtime: nvidia
    #esources:
      #slimits:
      #sd nvidia.com/gpu: 1
